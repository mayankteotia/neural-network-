{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c989c97e-7d4e-4c72-8944-44ff013d7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is deep learning, and how is it connected to artificial intelligence?\n",
    "Deep learning is a subset of machine learning (ML), which itself is a branch of artificial intelligence (AI). Deep learning involves using large artificial neural networks with multiple layers to model and solve complex problems like image recognition, natural language processing, and more. It allows machines to learn hierarchical representations of data, making it a powerful approach for AI applications.\n",
    "\n",
    "2. What is a neural network, and what are the different types of neural networks?\n",
    "A neural network is a computational model inspired by the human brain, consisting of interconnected layers of nodes (\"neurons\") that process and transform data.\n",
    "Different types of neural networks include:\n",
    "\n",
    "Feedforward Neural Networks (FNN)\n",
    "\n",
    "Convolutional Neural Networks (CNN)\n",
    "\n",
    "Recurrent Neural Networks (RNN)\n",
    "\n",
    "Long Short-Term Memory Networks (LSTM)\n",
    "\n",
    "Generative Adversarial Networks (GANs)\n",
    "\n",
    "Transformer Networks\n",
    "\n",
    "3. What is the mathematical structure of a neural network?\n",
    "A neural network is composed of layers of neurons. Mathematically:\n",
    "For each layer \n",
    "ð‘™\n",
    "l:\n",
    "\n",
    "ð‘§\n",
    "(\n",
    "ð‘™\n",
    ")\n",
    "=\n",
    "ð‘Š\n",
    "(\n",
    "ð‘™\n",
    ")\n",
    "ð‘Ž\n",
    "(\n",
    "ð‘™\n",
    "âˆ’\n",
    "1\n",
    ")\n",
    "+\n",
    "ð‘\n",
    "(\n",
    "ð‘™\n",
    ")\n",
    "z \n",
    "(l)\n",
    " =W \n",
    "(l)\n",
    " a \n",
    "(lâˆ’1)\n",
    " +b \n",
    "(l)\n",
    " \n",
    "ð‘Ž\n",
    "(\n",
    "ð‘™\n",
    ")\n",
    "=\n",
    "ðœŽ\n",
    "(\n",
    "ð‘§\n",
    "(\n",
    "ð‘™\n",
    ")\n",
    ")\n",
    "a \n",
    "(l)\n",
    " =Ïƒ(z \n",
    "(l)\n",
    " )\n",
    "Where:\n",
    "\n",
    "ð‘Š\n",
    "(\n",
    "ð‘™\n",
    ")\n",
    "W \n",
    "(l)\n",
    "  = weights matrix\n",
    "\n",
    "ð‘\n",
    "(\n",
    "ð‘™\n",
    ")\n",
    "b \n",
    "(l)\n",
    "  = bias vector\n",
    "\n",
    "ð‘Ž\n",
    "(\n",
    "ð‘™\n",
    "âˆ’\n",
    "1\n",
    ")\n",
    "a \n",
    "(lâˆ’1)\n",
    "  = activations from the previous layer\n",
    "\n",
    "ðœŽ\n",
    "Ïƒ = activation function\n",
    "\n",
    "4. What is an activation function, and why is it essential in neural networks?\n",
    "An activation function introduces non-linearity into the network, allowing it to learn complex patterns beyond simple linear relationships. Without it, the neural network would behave like a linear model, limiting its learning capacity.\n",
    "\n",
    "5. Could you list some common activation functions used in neural networks?\n",
    "\n",
    "Sigmoid\n",
    "\n",
    "Tanh (Hyperbolic tangent)\n",
    "\n",
    "ReLU (Rectified Linear Unit)\n",
    "\n",
    "Leaky ReLU\n",
    "\n",
    "ELU (Exponential Linear Unit)\n",
    "\n",
    "Softmax (for multi-class classification)\n",
    "\n",
    "6. What is a multilayer neural network?\n",
    "A multilayer neural network is a neural network with multiple hidden layers (layers between input and output). These hidden layers allow the network to learn more complex features and representations of the data.\n",
    "\n",
    "7. What is a loss function, and why is it crucial for neural network training?\n",
    "A loss function quantifies the difference between the network's predictions and the actual target values. It is essential because it guides the optimization process: the network adjusts its weights to minimize this loss during training.\n",
    "\n",
    "8. What are some common types of loss functions?\n",
    "\n",
    "Mean Squared Error (MSE) â€“ for regression tasks\n",
    "\n",
    "Mean Absolute Error (MAE)\n",
    "\n",
    "Cross-Entropy Loss â€“ for classification tasks\n",
    "\n",
    "Hinge Loss â€“ for support vector machines\n",
    "\n",
    "Categorical Cross-Entropy â€“ for multi-class classification\n",
    "\n",
    "9. How does a neural network learn?\n",
    "A neural network learns through a process called backpropagation:\n",
    "\n",
    "It performs a forward pass to compute predictions.\n",
    "\n",
    "It calculates the loss (error).\n",
    "\n",
    "It computes gradients of the loss w.r.t. weights (backpropagation).\n",
    "\n",
    "It updates the weights using an optimizer.\n",
    "\n",
    "This process is repeated iteratively over multiple epochs until the network converges.\n",
    "\n",
    "10. What is an optimizer in neural networks, and why is it necessary?\n",
    "An optimizer updates the network's weights based on gradients computed during backpropagation. It is necessary because it controls the learning processâ€”how the model adapts to dataâ€”and ensures faster convergence to a solution.\n",
    "\n",
    "11. Could you briefly describe some common optimizers?\n",
    "\n",
    "Stochastic Gradient Descent (SGD) â€“ basic optimizer, updates weights with small steps.\n",
    "\n",
    "Momentum â€“ adds momentum to SGD, helping escape local minima.\n",
    "\n",
    "AdaGrad â€“ adapts learning rates for each parameter.\n",
    "\n",
    "RMSProp â€“ handles non-stationary objectives by normalizing gradients.\n",
    "\n",
    "Adam â€“ combines momentum and adaptive learning rates (widely used).\n",
    "\n",
    "AdamW â€“ an improved version of Adam with weight decay for better generalization.\n",
    "\n",
    "12. Can you explain forward and backward propagation in a neural network?\n",
    "\n",
    "Forward Propagation: Input data flows through the network layer by layer, producing an output (prediction).\n",
    "\n",
    "Backward Propagation: The gradients of the loss with respect to each weight are computed by applying the chain rule (backpropagation). The gradients are used to update weights.\n",
    "\n",
    "13. What is weight initialization, and how does it impact training?\n",
    "Weight initialization sets the initial values for the network's weights before training. Good initialization (like Xavier or He initialization) helps avoid vanishing/exploding gradients and speeds up convergence. Poor initialization can lead to slow learning or network failure.\n",
    "\n",
    "14. What is the vanishing gradient problem in deep learning?\n",
    "In deep networks, gradients can shrink exponentially as they are propagated backward through layers. This can cause the networkâ€™s early layers to learn very slowly or not at all, hindering effective training.\n",
    "\n",
    "15. What is the exploding gradient problem?\n",
    "The exploding gradient problem occurs when gradients grow exponentially during backpropagation, causing numerical instability and very large weight updates that can destabilize learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a912b539-fc5d-40f3-abf2-3f2c93b45bb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#1.How do you create a simple perceptron for basic binary classification?\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#A perceptron can be created using Keras Sequential API:\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      8\u001b[0m     Dense(\u001b[38;5;241m1\u001b[39m, input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# For binary classification\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#1.How do you create a simple perceptron for basic binary classification?\n",
    "#A perceptron can be created using Keras Sequential API:\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(1, input_dim=2, activation='sigmoid')  # For binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b5104-f8c7-4143-9249-7756cd5acf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. How can you build a neural network with one hidden layer using Keras?\n",
    "Hereâ€™s a basic example:\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(8, input_dim=2, activation='relu'),  # Hidden layer with 8 neurons\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "3. How do you initialize weights using the Xavier (Glorot) initialization method in Keras?\n",
    "Use the kernel_initializer argument:\n",
    "\n",
    "\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(8, activation='relu', kernel_initializer=GlorotUniform())\n",
    "])\n",
    "4. How can you apply different activation functions in a neural network in Keras?\n",
    "You specify the activation in the Dense layer:\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(8, activation='tanh'),\n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "5. How do you add dropout to a neural network model to prevent overfitting?\n",
    "Use the Dropout layer:\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "6. How do you manually implement forward propagation in a simple neural network?\n",
    "For a simple network (without Keras), hereâ€™s an example:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Example weights and inputs\n",
    "X = np.array([0.5, 0.2])\n",
    "W = np.array([0.4, 0.3])\n",
    "b = 0.1\n",
    "\n",
    "# Forward propagation\n",
    "z = np.dot(X, W) + b\n",
    "a = sigmoid(z)\n",
    "\n",
    "print(a)\n",
    "7. How do you add batch normalization to a neural network model in Keras?\n",
    "Use the BatchNormalization layer:\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "8. How can you visualize the training process with accuracy and loss curves?\n",
    "Use Matplotlib and Keras history:\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "9. How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients?\n",
    "Use the clipnorm or clipvalue parameter in the optimizer:\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "10. How can you create a custom loss function in Keras?\n",
    "Define it as a Python function:\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true))  # Example: Mean Squared Error\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss)\n",
    "11. How can you visualize the structure of a neural network model in Keras?\n",
    "Use model.summary() or plot with plot_model:\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
